#! /bin/sh -e

# All lines beginning with `# DP:' are a description of the patch.
# DP: Description: @DESCRIPTION@
# DP: Related bugs: 
# DP: Dpatch author: 
# DP: Patch author: 
# DP: Upstream status: [In CVS | Debian-Specific | Pending | Not submitted ]
# DP: Status Details: 
# DP: Date: @DATE@

PATCHLEVEL=1

if [ $# -ne 2 ]; then
    echo >&2 "`basename $0`: script expects -patch|-unpatch as argument"
    exit 1
fi
case "$1" in
    -patch) patch -d "$2" -f --no-backup-if-mismatch -p$PATCHLEVEL < $0;;
    -unpatch) patch -d "$2" -f --no-backup-if-mismatch -R -p$PATCHLEVEL < $0;;
    *)
	echo >&2 "`basename $0`: script expects -patch|-unpatch as argument"
	exit 1
esac
exit 0

# append the patch here and adjust the -p? flag in the patch calls.
--- libc-orig/linuxthreads/descr.h	2005-02-16 10:14:12.000000000 -0500
+++ libc/linuxthreads/descr.h	2005-02-16 10:11:09.000000000 -0500
@@ -71,7 +71,7 @@
 /* Atomic counter made possible by compare_and_swap */
 struct pthread_atomic {
   long p_count;
-  int p_spinlock;
+  __atomic_lock_t p_spinlock;
 };
 
 
--- libc-orig/linuxthreads/oldsemaphore.c	2004-04-25 23:01:18.000000000 -0400
+++ libc/linuxthreads/oldsemaphore.c	2004-04-25 22:51:35.000000000 -0400
@@ -31,7 +31,7 @@
 
 typedef struct {
     long int sem_status;
-    int sem_spinlock;
+    __atomic_lock_t sem_spinlock;
 } old_sem_t;
 
 extern int __old_sem_init (old_sem_t *__sem, int __pshared, unsigned int __value);
--- libc-orig/linuxthreads/pt-machine.c	2002-08-26 18:39:45.000000000 -0400
+++ libc/linuxthreads/pt-machine.c	2003-12-08 21:24:59.000000000 -0500
@@ -19,7 +19,9 @@
 
 #define PT_EI
 
-extern long int testandset (int *spinlock);
+#include <pthread.h>
+
+extern long int testandset (__atomic_lock_t *spinlock);
 extern int __compare_and_swap (long int *p, long int oldval, long int newval);
 
 #include <pt-machine.h>
--- libc-orig/linuxthreads/pthread.c	2005-01-28 14:39:43.000000000 -0500
+++ libc/linuxthreads/pthread.c	2005-01-28 14:15:29.000000000 -0500
@@ -301,7 +301,7 @@
   pthread_descr self;
 
   /* First of all init __pthread_handles[0] and [1] if needed.  */
-# if __LT_SPINLOCK_INIT != 0
+# ifdef __LT_INITIALIZER_NOT_ZERO
   __pthread_handles[0].h_lock = __LOCK_INITIALIZER;
   __pthread_handles[1].h_lock = __LOCK_INITIALIZER;
 # endif
@@ -371,7 +371,7 @@
 # endif
   /* self->p_start_args need not be initialized, it's all zero.  */
   self->p_userstack = 1;
-# if __LT_SPINLOCK_INIT != 0
+# ifdef __LT_INITIALIZER_NOT_ZERO 
   self->p_resume_count = (struct pthread_atomic) __ATOMIC_INITIALIZER;
 # endif
   self->p_alloca_cutoff = __MAX_ALLOCA_CUTOFF;
@@ -385,7 +385,7 @@
 #else  /* USE_TLS */
 
   /* First of all init __pthread_handles[0] and [1].  */
-# if __LT_SPINLOCK_INIT != 0
+# ifdef __LT_INITIALIZER_NOT_ZERO
   __pthread_handles[0].h_lock = __LOCK_INITIALIZER;
   __pthread_handles[1].h_lock = __LOCK_INITIALIZER;
 # endif
@@ -688,7 +688,7 @@
 # endif
   mgr->p_start_args = (struct pthread_start_args) PTHREAD_START_ARGS_INITIALIZER(__pthread_manager);
   mgr->p_nr = 1;
-# if __LT_SPINLOCK_INIT != 0
+# ifdef __LT_INITIALIZER_NOT_ZERO
   self->p_resume_count = (struct pthread_atomic) __ATOMIC_INITIALIZER;
 # endif
   mgr->p_alloca_cutoff = PTHREAD_STACK_MIN / 4;
--- libc-orig/linuxthreads/spinlock.c	2004-01-30 14:13:45.000000000 -0500
+++ libc/linuxthreads/spinlock.c	2004-01-30 10:47:31.000000000 -0500
@@ -24,9 +24,9 @@
 #include "spinlock.h"
 #include "restart.h"
 
-static void __pthread_acquire(int * spinlock);
+static void __pthread_acquire(__atomic_lock_t * spinlock);
 
-static inline void __pthread_release(int * spinlock)
+static inline void __pthread_release(__atomic_lock_t * spinlock)
 {
   WRITE_MEMORY_BARRIER();
   *spinlock = __LT_SPINLOCK_INIT;
@@ -269,11 +269,11 @@
 struct wait_node {
   struct wait_node *next;	/* Next node in null terminated linked list */
   pthread_descr thr;		/* The thread waiting with this node */
-  int abandoned;		/* Atomic flag */
+  __atomic_lock_t abandoned;	/* Atomic flag */
 };
 
 static long wait_node_free_list;
-static int wait_node_free_list_spinlock;
+__pthread_lock_define_initialized(static, wait_node_free_list_spinlock);
 
 /* Allocate a new node from the head of the free list using an atomic
    operation, or else using malloc if that list is empty.  A fundamental
@@ -376,7 +376,7 @@
       if (self == NULL)
 	self = thread_self();
 
-      wait_node.abandoned = 0;
+      wait_node.abandoned = __LT_SPINLOCK_INIT;
       wait_node.next = (struct wait_node *) lock->__status;
       wait_node.thr = self;
       lock->__status = (long) &wait_node;
@@ -402,7 +402,7 @@
       wait_node.thr = self;
       newstatus = (long) &wait_node;
     }
-    wait_node.abandoned = 0;
+    wait_node.abandoned = __LT_SPINLOCK_INIT;
     wait_node.next = (struct wait_node *) oldstatus;
     /* Make sure the store in wait_node.next completes before performing
        the compare-and-swap */
@@ -451,7 +451,7 @@
       if (self == NULL)
 	self = thread_self();
 
-      p_wait_node->abandoned = 0;
+      p_wait_node->abandoned = __LT_SPINLOCK_INIT;
       p_wait_node->next = (struct wait_node *) lock->__status;
       p_wait_node->thr = self;
       lock->__status = (long) p_wait_node;
@@ -474,7 +474,7 @@
       p_wait_node->thr = self;
       newstatus = (long) p_wait_node;
     }
-    p_wait_node->abandoned = 0;
+    p_wait_node->abandoned = __LT_SPINLOCK_INIT;
     p_wait_node->next = (struct wait_node *) oldstatus;
     /* Make sure the store in wait_node.next completes before performing
        the compare-and-swap */
@@ -574,7 +574,7 @@
     while (p_node != (struct wait_node *) 1) {
       int prio;
 
-      if (p_node->abandoned) {
+      if (lock_held(&p_node->abandoned)) {
 	/* Remove abandoned node. */
 #if defined TEST_FOR_COMPARE_AND_SWAP
 	if (!__pthread_has_cas)
@@ -662,7 +662,7 @@
 #if !defined HAS_COMPARE_AND_SWAP || defined TEST_FOR_COMPARE_AND_SWAP
 
 int __pthread_compare_and_swap(long * ptr, long oldval, long newval,
-                               int * spinlock)
+                               __atomic_lock_t * spinlock)
 {
   int res;
 
@@ -699,7 +699,7 @@
    - When nanosleep() returns, we try again, doing MAX_SPIN_COUNT
      sched_yield(), then sleeping again if needed. */
 
-static void __pthread_acquire(int * spinlock)
+static void __pthread_acquire(__atomic_lock_t * spinlock)	
 {
   int cnt = 0;
   struct timespec tm;
--- libc-orig/linuxthreads/spinlock.h	2003-07-31 15:16:04.000000000 -0400
+++ libc/linuxthreads/spinlock.h	2003-12-08 21:24:59.000000000 -0500
@@ -33,14 +33,28 @@
 #endif
 #endif
 
+/* Define lock_held for all arches that don't need a modified copy. */
+#ifndef __LT_INITIALIZER_NOT_ZERO
+# define lock_held(p) *(p)
+#endif
+
+/* Initliazers for possibly complex structures */
+#ifdef __LT_INITIALIZER_NOT_ZERO
+# define __pthread_lock_define_initialized(CLASS,NAME) \
+	CLASS __atomic_lock_t NAME = __LT_SPINLOCK_ALT_INIT
+#else
+# define __pthread_lock_define_initialized(CLASS,NAME) \
+	CLASS __atomic_lock_t NAME
+#endif
+
 #if defined(TEST_FOR_COMPARE_AND_SWAP)
 
 extern int __pthread_has_cas;
 extern int __pthread_compare_and_swap(long * ptr, long oldval, long newval,
-                                      int * spinlock);
+                                      __atomic_lock_t * spinlock);
 
 static inline int compare_and_swap(long * ptr, long oldval, long newval,
-                                   int * spinlock)
+                                   __atomic_lock_t * spinlock)
 {
   if (__builtin_expect (__pthread_has_cas, 1))
     return __compare_and_swap(ptr, oldval, newval);
@@ -58,7 +72,7 @@
 
 static inline int
 compare_and_swap_with_release_semantics (long * ptr, long oldval,
-					 long newval, int * spinlock)
+					 long newval, __atomic_lock_t * spinlock)
 {
   return __compare_and_swap_with_release_semantics (ptr, oldval,
 						    newval);
@@ -67,7 +81,7 @@
 #endif
 
 static inline int compare_and_swap(long * ptr, long oldval, long newval,
-                                   int * spinlock)
+                                   __atomic_lock_t * spinlock)
 {
   return __compare_and_swap(ptr, oldval, newval);
 }
@@ -75,10 +89,10 @@
 #else
 
 extern int __pthread_compare_and_swap(long * ptr, long oldval, long newval,
-                                      int * spinlock);
+                                      __atomic_lock_t * spinlock);
 
 static inline int compare_and_swap(long * ptr, long oldval, long newval,
-                                   int * spinlock)
+                                   __atomic_lock_t * spinlock)
 {
   return __pthread_compare_and_swap(ptr, oldval, newval, spinlock);
 }
--- libc-orig/linuxthreads/sysdeps/hppa/pspinlock.c	2002-08-26 18:39:51.000000000 -0400
+++ libc/linuxthreads/sysdeps/hppa/pspinlock.c	2004-08-15 14:22:02.000000000 -0400
@@ -24,13 +24,10 @@
 int
 __pthread_spin_lock (pthread_spinlock_t *lock)
 {
-  unsigned int val;
+  volatile unsigned int *addr = __ldcw_align (lock);
 
-  do
-    asm volatile ("ldcw %1,%0"
-		  : "=r" (val), "=m" (*lock)
-		  : "m" (*lock));
-  while (!val);
+  while (__ldcw (addr) == 0)
+    while (*addr == 0) ;
 
   return 0;
 }
@@ -40,13 +37,9 @@
 int
 __pthread_spin_trylock (pthread_spinlock_t *lock)
 {
-  unsigned int val;
+  volatile unsigned int *a = __ldcw_align (lock);
 
-  asm volatile ("ldcw %1,%0"
-		: "=r" (val), "=m" (*lock)
-		: "m" (*lock));
-
-  return val ? 0 : EBUSY;
+  return __ldcw (a) ? 0 : EBUSY;
 }
 weak_alias (__pthread_spin_trylock, pthread_spin_trylock)
 
@@ -54,7 +47,11 @@
 int
 __pthread_spin_unlock (pthread_spinlock_t *lock)
 {
-  *lock = 1;
+  volatile unsigned int *a = __ldcw_align (lock);
+  int tmp = 1;
+  /* This should be a memory barrier to newer compilers */
+  __asm__ __volatile__ ("stw,ma %1,0(%0)"
+                        : : "r" (a), "r" (tmp) : "memory");           
   return 0;
 }
 weak_alias (__pthread_spin_unlock, pthread_spin_unlock)
@@ -66,7 +63,11 @@
   /* We can ignore the `pshared' parameter.  Since we are busy-waiting
      all processes which can access the memory location `lock' points
      to can use the spinlock.  */
-  *lock = 1;
+  volatile unsigned int *a = __ldcw_align (lock);
+  int tmp = 1;
+  /* This should be a memory barrier to newer compilers */
+  __asm__ __volatile__ ("stw,ma %1,0(%0)"
+                        : : "r" (a), "r" (tmp) : "memory");           
   return 0;
 }
 weak_alias (__pthread_spin_init, pthread_spin_init)
--- libc-orig/linuxthreads/sysdeps/hppa/pt-machine.h	2003-07-31 15:15:42.000000000 -0400
+++ libc/linuxthreads/sysdeps/hppa/pt-machine.h	2004-08-23 14:39:23.000000000 -0400
@@ -22,41 +22,103 @@
 #ifndef _PT_MACHINE_H
 #define _PT_MACHINE_H   1
 
+#include <sys/types.h>
 #include <bits/initspin.h>
 
 #ifndef PT_EI
 # define PT_EI extern inline __attribute__ ((always_inline))
 #endif
 
-extern long int testandset (int *spinlock);
-extern int __compare_and_swap (long int *p, long int oldval, long int newval);
+extern inline long int testandset (__atomic_lock_t *spinlock);
+extern inline int __compare_and_swap (long int *p, long int oldval, long int newval);
+extern inline int lock_held (__atomic_lock_t *spinlock); 
+extern inline int __load_and_clear (__atomic_lock_t *spinlock);
 
 /* Get some notion of the current stack.  Need not be exactly the top
    of the stack, just something somewhere in the current frame.  */
 #define CURRENT_STACK_FRAME  stack_pointer
 register char * stack_pointer __asm__ ("%r30");
 
+/* Get/Set thread-specific pointer.  We have to call into the kernel to
+ * modify it, but we can read it in user mode.  */
+
+#define THREAD_SELF __get_cr27()
+
+static inline struct _pthread_descr_struct * __get_cr27(void)
+{
+	long cr27;
+	asm("mfctl %%cr27, %0" : "=r" (cr27) : );
+	return (struct _pthread_descr_struct *) cr27;
+}
+
+#define INIT_THREAD_SELF(descr, nr) __set_cr27(descr)
+
+static inline void __set_cr27(struct _pthread_descr_struct * cr27)
+{
+	asm(
+		"ble	0xe0(%%sr2, %%r0)\n\t"
+		"copy	%0, %%r26"
+	 : : "r" (cr27) : "r26" );
+}
+
+/* We want the OS to assign stack addresses.  */
+#define FLOATING_STACKS	1
+#define ARCH_STACK_MAX_SIZE	8*1024*1024
 
 /* The hppa only has one atomic read and modify memory operation,
    load and clear, so hppa spinlocks must use zero to signify that
-   someone is holding the lock.  */
+   someone is holding the lock.  The address used for the ldcw
+   semaphore must be 16-byte aligned.  */
+#define __ldcw(a) ({ \
+  unsigned int __ret;							\
+  __asm__ __volatile__("ldcw 0(%1),%0"					\
+                      : "=r" (__ret) : "r" (a) : "memory");		\
+  __ret;								\
+})
+
+/* Strongly ordered lock reset */
+#define __lock_reset(lock_addr, tmp) ({						\
+	__asm__ __volatile__ ("stw,ma %1,0(%0)"					\
+				: : "r" (lock_addr), "r" (tmp) : "memory"); 	\
+    })
+
+/* Because malloc only guarantees 8-byte alignment for malloc'd data,
+   and GCC only guarantees 8-byte alignment for stack locals, we can't
+   be assured of 16-byte alignment for atomic lock data even if we
+   specify "__attribute ((aligned(16)))" in the type declaration.  So,
+   we use a struct containing an array of four ints for the atomic lock
+   type and dynamically select the 16-byte aligned int from the array
+   for the semaphore.  */
+#define __PA_LDCW_ALIGNMENT 16
+#define __ldcw_align(a) ({ \
+  volatile unsigned int __ret = (unsigned int) a;			\
+  if ((__ret & ~(__PA_LDCW_ALIGNMENT - 1)) < (unsigned int) a)		\
+    __ret = (__ret & ~(__PA_LDCW_ALIGNMENT - 1)) + __PA_LDCW_ALIGNMENT; \
+  (unsigned int *) __ret;						\
+})
 
-#define xstr(s) str(s)
-#define str(s) #s
 /* Spinlock implementation; required.  */
-PT_EI long int
-testandset (int *spinlock)
+PT_EI int
+__load_and_clear (__atomic_lock_t *spinlock)
 {
-  int ret;
+  volatile unsigned int *a = __ldcw_align (spinlock);
 
-  __asm__ __volatile__(
-       "ldcw 0(%2),%0"
-       : "=r"(ret), "=m"(*spinlock)
-       : "r"(spinlock));
+  return __ldcw (a);
+}
 
-  return ret == 0;
+/* Emulate testandset */
+PT_EI long int
+testandset (__atomic_lock_t *spinlock)
+{
+  return (__load_and_clear(spinlock) == 0);
 }
-#undef str
-#undef xstr
 
+PT_EI int
+lock_held (__atomic_lock_t *spinlock)
+{
+  volatile unsigned int *a = __ldcw_align (spinlock);
+
+  return *a == 0;
+}
+		
 #endif /* pt-machine.h */
--- libc-orig/linuxthreads/sysdeps/pthread/bits/initspin.h	2002-08-26 18:39:44.000000000 -0400
+++ libc/linuxthreads/sysdeps/pthread/bits/initspin.h	2004-02-23 09:36:18.000000000 -0500
@@ -23,6 +23,7 @@
 #define __LT_SPINLOCK_INIT 0
 
 /* Macros for lock initializers, using the above definition. */
-#define __LOCK_INITIALIZER { 0, __LT_SPINLOCK_INIT }
+#define __LOCK_INITIALIZER ((struct _pthread_fastlock){ 0, __LT_SPINLOCK_INIT })
+#define __LOCK_ALT_INITIALIZER { 0, __LT_SPINLOCK_INIT }
 #define __ALT_LOCK_INITIALIZER { 0, __LT_SPINLOCK_INIT }
 #define __ATOMIC_INITIALIZER { 0, __LT_SPINLOCK_INIT }
--- libc-orig/linuxthreads/sysdeps/pthread/bits/libc-lock.h	2003-09-23 00:33:20.000000000 -0400
+++ libc/linuxthreads/sysdeps/pthread/bits/libc-lock.h	2003-12-08 21:25:00.000000000 -0500
@@ -71,12 +71,12 @@
    initialized locks must be set to one due to the lack of normal
    atomic operations.) */
 
-#if __LT_SPINLOCK_INIT == 0
+#ifdef __LT_INITIALIZER_NOT_ZERO
 #  define __libc_lock_define_initialized(CLASS,NAME) \
-  CLASS __libc_lock_t NAME;
+  CLASS __libc_lock_t NAME = PTHREAD_MUTEX_INITIALIZER;
 #else
 #  define __libc_lock_define_initialized(CLASS,NAME) \
-  CLASS __libc_lock_t NAME = PTHREAD_MUTEX_INITIALIZER;
+  CLASS __libc_lock_t NAME;
 #endif
 
 #define __libc_rwlock_define_initialized(CLASS,NAME) \
--- libc-orig/linuxthreads/sysdeps/pthread/bits/pthreadtypes.h	2004-09-17 12:24:47.000000000 -0400
+++ libc/linuxthreads/sysdeps/pthread/bits/pthreadtypes.h	2004-09-17 12:24:19.000000000 -0400
@@ -22,12 +22,14 @@
 #define __need_schedparam
 #include <bits/sched.h>
 
+typedef int __atomic_lock_t;
+
 /* Fast locks (not abstract because mutexes and conditions aren't abstract). */
 struct _pthread_fastlock
 {
-  long int __status;   /* "Free" or "taken" or head of waiting list */
-  int __spinlock;      /* Used by compare_and_swap emulation. Also,
-			  adaptive SMP lock stores spin count here. */
+  long int __status;		/* "Free" or "taken" or head of waiting list */
+  __atomic_lock_t __spinlock;	/* Used by compare_and_swap emulation. Also,
+				   adaptive SMP lock stores spin count here. */
 };
 
 #ifndef _PTHREAD_DESCR_DEFINED
--- libc-orig/linuxthreads/sysdeps/pthread/pthread.h	2004-09-21 17:55:20.000000000 -0400
+++ libc/linuxthreads/sysdeps/pthread/pthread.h	2004-09-21 17:55:03.000000000 -0400
@@ -31,26 +31,26 @@
 /* Initializers.  */
 
 #define PTHREAD_MUTEX_INITIALIZER \
-  {0, 0, 0, PTHREAD_MUTEX_TIMED_NP, __LOCK_INITIALIZER}
+  {0, 0, 0, PTHREAD_MUTEX_TIMED_NP, __LOCK_ALT_INITIALIZER}
 #ifdef __USE_GNU
 # define PTHREAD_RECURSIVE_MUTEX_INITIALIZER_NP \
-  {0, 0, 0, PTHREAD_MUTEX_RECURSIVE_NP, __LOCK_INITIALIZER}
+  {0, 0, 0, PTHREAD_MUTEX_RECURSIVE_NP, __LOCK_ALT_INITIALIZER}
 # define PTHREAD_ERRORCHECK_MUTEX_INITIALIZER_NP \
-  {0, 0, 0, PTHREAD_MUTEX_ERRORCHECK_NP, __LOCK_INITIALIZER}
+  {0, 0, 0, PTHREAD_MUTEX_ERRORCHECK_NP, __LOCK_ALT_INITIALIZER}
 # define PTHREAD_ADAPTIVE_MUTEX_INITIALIZER_NP \
-  {0, 0, 0, PTHREAD_MUTEX_ADAPTIVE_NP, __LOCK_INITIALIZER}
+  {0, 0, 0, PTHREAD_MUTEX_ADAPTIVE_NP, __LOCK_ALT_INITIALIZER}
 #endif
 
-#define PTHREAD_COND_INITIALIZER {__LOCK_INITIALIZER, 0, "", 0}
+#define PTHREAD_COND_INITIALIZER {__LOCK_ALT_INITIALIZER, 0, "", 0}
 
 #if defined __USE_UNIX98 || defined __USE_XOPEN2K
 # define PTHREAD_RWLOCK_INITIALIZER \
-  { __LOCK_INITIALIZER, 0, NULL, NULL, NULL,				      \
+  { __LOCK_ALT_INITIALIZER, 0, NULL, NULL, NULL,			      \
     PTHREAD_RWLOCK_DEFAULT_NP, PTHREAD_PROCESS_PRIVATE }
 #endif
 #ifdef __USE_GNU
 # define PTHREAD_RWLOCK_WRITER_NONRECURSIVE_INITIALIZER_NP \
-  { __LOCK_INITIALIZER, 0, NULL, NULL, NULL,				      \
+  { __LOCK_ALT_INITIALIZER, 0, NULL, NULL, NULL,			      \
     PTHREAD_RWLOCK_PREFER_WRITER_NONRECURSIVE_NP, PTHREAD_PROCESS_PRIVATE }
 #endif
 
--- libc-orig/linuxthreads/sysdeps/unix/sysv/linux/hppa/bits/initspin.h	2002-08-26 18:39:55.000000000 -0400
+++ libc/linuxthreads/sysdeps/unix/sysv/linux/hppa/bits/initspin.h	2004-02-23 09:35:37.000000000 -0500
@@ -19,9 +19,23 @@
 
 /* Initial value of a spinlock.  PA-RISC only implements atomic load
    and clear so this must be non-zero. */
-#define __LT_SPINLOCK_INIT 1
+#define __LT_SPINLOCK_INIT ((__atomic_lock_t) { { 1, 1, 1, 1 } })
+
+/* Initialize global spinlocks without cast, generally macro wrapped */
+#define __LT_SPINLOCK_ALT_INIT { { 1, 1, 1, 1 } }
+
+/* Macros for lock initializers, not using the above definition.
+   The above definition is not used in the case that static initializers
+   use this value. */
+#define __LOCK_ALT_INITIALIZER { __LT_SPINLOCK_ALT_INIT, 0 }
+
+/* Used to initialize _pthread_fastlock's in non-static case */
+#define __LOCK_INITIALIZER ((struct _pthread_fastlock){ __LT_SPINLOCK_INIT, 0 })
+
+/* Used in pthread_atomic initialization */
+#define __ATOMIC_INITIALIZER { 0, __LT_SPINLOCK_ALT_INIT }
+
+/* Tell the rest of the code that the initializer is non-zero without
+   explaining it's internal structure */
+#define __LT_INITIALIZER_NOT_ZERO
 
-/* Macros for lock initializers, using the above definition. */
-#define __LOCK_INITIALIZER { 0, __LT_SPINLOCK_INIT }
-#define __ALT_LOCK_INITIALIZER { 0, __LT_SPINLOCK_INIT }
-#define __ATOMIC_INITIALIZER { 0, __LT_SPINLOCK_INIT }
--- libc-orig/linuxthreads/sysdeps/unix/sysv/linux/hppa/bits/pthreadtypes.h	1969-12-31 19:00:00.000000000 -0500
+++ libc/linuxthreads/sysdeps/unix/sysv/linux/hppa/bits/pthreadtypes.h	2003-12-08 21:25:00.000000000 -0500
@@ -0,0 +1,160 @@
+/* Linuxthreads - a simple clone()-based implementation of Posix        */
+/* threads for Linux.                                                   */
+/* Copyright (C) 1996 Xavier Leroy (Xavier.Leroy@inria.fr)              */
+/*                                                                      */
+/* This program is free software; you can redistribute it and/or        */
+/* modify it under the terms of the GNU Library General Public License  */
+/* as published by the Free Software Foundation; either version 2       */
+/* of the License, or (at your option) any later version.               */
+/*                                                                      */
+/* This program is distributed in the hope that it will be useful,      */
+/* but WITHOUT ANY WARRANTY; without even the implied warranty of       */
+/* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the        */
+/* GNU Library General Public License for more details.                 */
+
+#if !defined _BITS_TYPES_H && !defined _PTHREAD_H
+# error "Never include <bits/pthreadtypes.h> directly; use <sys/types.h> instead."
+#endif
+
+#ifndef _BITS_PTHREADTYPES_H
+#define _BITS_PTHREADTYPES_H	1
+
+#define __need_schedparam
+#include <bits/sched.h>
+
+/* We need 128-bit alignment for the ldcw semaphore.  At most, we are
+   assured of 64-bit alignment for stack locals and malloc'd data.  Thus,
+   we use a struct with four ints for the atomic lock type.  The locking
+   code will figure out which of the four to use for the ldcw semaphore.  */
+typedef volatile struct {
+  int lock[4];
+} __attribute__ ((aligned(16))) __atomic_lock_t;
+
+/* Fast locks (not abstract because mutexes and conditions aren't abstract). */
+struct _pthread_fastlock
+{
+  __atomic_lock_t __spinlock;	/* Used by compare_and_swap emulation.  Also,
+				   adaptive SMP lock stores spin count here. */
+  long int __status;		/* "Free" or "taken" or head of waiting list */
+};
+
+#ifndef _PTHREAD_DESCR_DEFINED
+/* Thread descriptors */
+typedef struct _pthread_descr_struct *_pthread_descr;
+# define _PTHREAD_DESCR_DEFINED
+#endif
+
+
+/* Attributes for threads.  */
+typedef struct __pthread_attr_s
+{
+  int __detachstate;
+  int __schedpolicy;
+  struct __sched_param __schedparam;
+  int __inheritsched;
+  int __scope;
+  size_t __guardsize;
+  int __stackaddr_set;
+  void *__stackaddr;
+  size_t __stacksize;
+} pthread_attr_t;
+
+
+/* Conditions (not abstract because of PTHREAD_COND_INITIALIZER */
+
+#ifdef __GLIBC_HAVE_LONG_LONG
+__extension__ typedef long long __pthread_cond_align_t;
+#else
+typedef long __pthread_cond_align_t;
+#endif
+
+typedef struct
+{
+  struct _pthread_fastlock __c_lock; /* Protect against concurrent access */
+  _pthread_descr __c_waiting;        /* Threads waiting on this condition */
+  char __padding[48 - sizeof (struct _pthread_fastlock)
+		 - sizeof (_pthread_descr) - sizeof (__pthread_cond_align_t)];
+  __pthread_cond_align_t __align;
+} pthread_cond_t;
+
+
+/* Attribute for conditionally variables.  */
+typedef struct
+{
+  int __dummy;
+} pthread_condattr_t;
+
+/* Keys for thread-specific data */
+typedef unsigned int pthread_key_t;
+
+
+/* Mutexes (not abstract because of PTHREAD_MUTEX_INITIALIZER).  */
+/* (The layout is unnatural to maintain binary compatibility
+    with earlier releases of LinuxThreads.) */
+typedef struct
+{
+  int __m_reserved;               /* Reserved for future use */
+  int __m_count;                  /* Depth of recursive locking */
+  _pthread_descr __m_owner;       /* Owner thread (if recursive or errcheck) */
+  int __m_kind;                   /* Mutex kind: fast, recursive or errcheck */
+  struct _pthread_fastlock __m_lock; /* Underlying fast lock */
+} pthread_mutex_t;
+
+
+/* Attribute for mutex.  */
+typedef struct
+{
+  int __mutexkind;
+} pthread_mutexattr_t;
+
+
+/* Once-only execution */
+typedef int pthread_once_t;
+
+
+#ifdef __USE_UNIX98
+/* Read-write locks.  */
+typedef struct _pthread_rwlock_t
+{
+  struct _pthread_fastlock __rw_lock; /* Lock to guarantee mutual exclusion */
+  int __rw_readers;                   /* Number of readers */
+  _pthread_descr __rw_writer;         /* Identity of writer, or NULL if none */
+  _pthread_descr __rw_read_waiting;   /* Threads waiting for reading */
+  _pthread_descr __rw_write_waiting;  /* Threads waiting for writing */
+  int __rw_kind;                      /* Reader/Writer preference selection */
+  int __rw_pshared;                   /* Shared between processes or not */
+} pthread_rwlock_t;
+
+
+/* Attribute for read-write locks.  */
+typedef struct
+{
+  int __lockkind;
+  int __pshared;
+} pthread_rwlockattr_t;
+#endif
+
+#ifdef __USE_XOPEN2K
+/* POSIX spinlock data type.  */
+typedef __atomic_lock_t pthread_spinlock_t;
+
+/* POSIX barrier. */
+typedef struct {
+  struct _pthread_fastlock __ba_lock; /* Lock to guarantee mutual exclusion */
+  int __ba_required;                  /* Threads needed for completion */
+  int __ba_present;                   /* Threads waiting */
+  _pthread_descr __ba_waiting;        /* Queue of waiting threads */
+} pthread_barrier_t;
+
+/* barrier attribute */
+typedef struct {
+  int __pshared;
+} pthread_barrierattr_t;
+
+#endif
+
+
+/* Thread identifiers */
+typedef unsigned long int pthread_t;
+
+#endif	/* bits/pthreadtypes.h */
--- libc-orig/linuxthreads/sysdeps/unix/sysv/linux/hppa/sysdep-cancel.h	2003-10-10 21:28:08.000000000 -0400
+++ libc/linuxthreads/sysdeps/unix/sysv/linux/hppa/sysdep-cancel.h	2004-09-22 20:05:32.000000000 -0400
@@ -29,61 +29,109 @@
 #  define NO_ERROR -0x1000
 # endif
 
+/* The syscall cancellation mechanism requires userspace
+   assistance, the following code does roughly this:
+
+   	do arguments (read arg5 and arg6 to registers)
+	setup frame
+	
+	check if there are threads, yes jump to pseudo_cancel
+	
+	unthreaded:
+		syscall
+		check syscall return (jump to pre_end)
+		set errno
+		set return to -1
+		(jump to pre_end)
+		
+	pseudo_cancel:
+		cenable
+		syscall
+		cdisable
+		check syscall return (jump to pre_end)
+		set errno
+		set return to -1
+		
+	pre_end
+		restore stack
+	
+	It is expected that 'ret' and 'END' macros will
+	append an 'undo arguments' and 'return' to the 
+	this PSEUDO macro. */
+   
 # undef PSEUDO
 # define PSEUDO(name, syscall_name, args)				\
-  ENTRY (name)								\
-    SINGLE_THREAD_P					ASM_LINE_SEP	\
-    cmpib,<> 0,%ret0,Lpseudo_cancel			ASM_LINE_SEP	\
-    nop							ASM_LINE_SEP	\
-    DO_CALL(syscall_name, args)				ASM_LINE_SEP	\
-    /* DONE! */						ASM_LINE_SEP	\
-    bv 0(2)						ASM_LINE_SEP	\
-    nop							ASM_LINE_SEP	\
-  Lpseudo_cancel:					ASM_LINE_SEP	\
-    /* store return ptr */				ASM_LINE_SEP	\
-    stw %rp, -20(%sr0,%sp)				ASM_LINE_SEP	\
-    /* save syscall args */				ASM_LINE_SEP	\
-    PUSHARGS_##args /* MACRO */				ASM_LINE_SEP	\
-    STW_PIC						ASM_LINE_SEP	\
-    CENABLE /* FUNC CALL */				ASM_LINE_SEP	\
-    ldo 64(%sp), %sp					ASM_LINE_SEP	\
-    ldo -64(%sp), %sp					ASM_LINE_SEP	\
-    LDW_PIC						ASM_LINE_SEP	\
-    /* restore syscall args */				ASM_LINE_SEP	\
-    POPARGS_##args					ASM_LINE_SEP	\
-    /* save r4 in arg0 stack slot */			ASM_LINE_SEP	\
-    stw %r4, -36(%sr0,%sp)				ASM_LINE_SEP	\
-    /* save mask from cenable */			ASM_LINE_SEP	\
-    copy %ret0, %r4					ASM_LINE_SEP	\
-    ble 0x100(%sr2,%r0)					ASM_LINE_SEP    \
-    ldi SYS_ify (syscall_name), %r20			ASM_LINE_SEP	\
-    LDW_PIC						ASM_LINE_SEP	\
-    /* pass mask as arg0 to cdisable */			ASM_LINE_SEP	\
-    copy %r4, %r26					ASM_LINE_SEP	\
-    copy %ret0, %r4					ASM_LINE_SEP	\
-    CDISABLE						ASM_LINE_SEP	\
-    ldo 64(%sp), %sp					ASM_LINE_SEP	\
-    ldo -64(%sp), %sp					ASM_LINE_SEP	\
-    LDW_PIC						ASM_LINE_SEP	\
-    /* compare error */					ASM_LINE_SEP	\
-    ldi NO_ERROR,%r1					ASM_LINE_SEP	\
-    /* branch if no error */				ASM_LINE_SEP	\
-    cmpb,>>=,n %r1,%r4,Lpre_end				ASM_LINE_SEP	\
-    nop							ASM_LINE_SEP	\
-    SYSCALL_ERROR_HANDLER				ASM_LINE_SEP	\
-    ldo 64(%sp), %sp					ASM_LINE_SEP	\
-    ldo -64(%sp), %sp					ASM_LINE_SEP	\
-    /* No need to LDW_PIC */				ASM_LINE_SEP	\
-    /* make syscall res value positive */		ASM_LINE_SEP	\
-    sub %r0, %r4, %r4					ASM_LINE_SEP	\
-    /* store into errno location */			ASM_LINE_SEP	\
-    stw %r4, 0(%sr0,%ret0)				ASM_LINE_SEP	\
-    /* return -1 */					ASM_LINE_SEP	\
-    ldo -1(%r0), %ret0					ASM_LINE_SEP	\
-  Lpre_end:						ASM_LINE_SEP	\
-    ldw -20(%sr0,%sp), %rp             			ASM_LINE_SEP	\
-    /* No need to LDW_PIC */				ASM_LINE_SEP	\
-    ldw -36(%sr0,%sp), %r4				ASM_LINE_SEP
+	ENTRY (name)							\
+	DOARGS_##args					ASM_LINE_SEP	\
+	copy TREG, %r1					ASM_LINE_SEP	\
+	copy %sp, TREG					ASM_LINE_SEP	\
+	stwm %r1, 64(%sp)				ASM_LINE_SEP	\
+	stw %rp, -20(%sp)				ASM_LINE_SEP	\
+	stw TREG, -4(%sp)				ASM_LINE_SEP	\
+	/* Done setting up frame, continue... */	ASM_LINE_SEP	\
+	SINGLE_THREAD_P					ASM_LINE_SEP	\
+	cmpib,<>,n 0,%ret0,L(pseudo_cancel)		ASM_LINE_SEP	\
+L(unthreaded):						ASM_LINE_SEP	\
+	/* Save r19 */					ASM_LINE_SEP	\
+	SAVE_PIC(TREG)					ASM_LINE_SEP	\
+	/* Do syscall, delay loads # */			ASM_LINE_SEP	\
+	ble  0x100(%sr2,%r0)				ASM_LINE_SEP	\
+	ldi SYS_ify (syscall_name), %r20 /* delay */	ASM_LINE_SEP	\
+	ldi NO_ERROR,%r1				ASM_LINE_SEP	\
+	cmpb,>>=,n %r1,%ret0,L(pre_end)			ASM_LINE_SEP	\
+	/* Restore r19 from TREG */			ASM_LINE_SEP	\
+	LOAD_PIC(TREG) /* delay */			ASM_LINE_SEP	\
+	SYSCALL_ERROR_HANDLER				ASM_LINE_SEP	\
+	/* Use TREG for temp storage */			ASM_LINE_SEP	\
+	copy %ret0, TREG /* delay */			ASM_LINE_SEP	\
+	/* OPTIMIZE: Don't reload r19 */		ASM_LINE_SEP	\
+	/* do a -1*syscall_ret0 */			ASM_LINE_SEP	\
+	sub %r0, TREG, TREG				ASM_LINE_SEP	\
+	/* Store into errno location */			ASM_LINE_SEP	\
+	stw TREG, 0(%sr0,%ret0)				ASM_LINE_SEP	\
+	b L(pre_end)					ASM_LINE_SEP	\
+	/* return -1 as error */			ASM_LINE_SEP	\
+	ldo -1(%r0), %ret0 /* delay */			ASM_LINE_SEP	\
+L(pseudo_cancel):					ASM_LINE_SEP	\
+	PUSHARGS_##args /* Save args */			ASM_LINE_SEP	\
+	/* Save r19 into TREG */			ASM_LINE_SEP	\
+	CENABLE /* FUNC CALL */				ASM_LINE_SEP	\
+	SAVE_PIC(TREG) /* delay */			ASM_LINE_SEP	\
+	/* restore syscall args */			ASM_LINE_SEP	\
+	POPARGS_##args					ASM_LINE_SEP	\
+	/* save mask from cenable (use stub rp slot) */	ASM_LINE_SEP	\
+	stw %ret0, -24(%sp)				ASM_LINE_SEP	\
+	/* ... SYSCALL ... */				ASM_LINE_SEP	\
+	ble 0x100(%sr2,%r0)				ASM_LINE_SEP    \
+	ldi SYS_ify (syscall_name), %r20 /* delay */	ASM_LINE_SEP	\
+	/* ............... */				ASM_LINE_SEP	\
+	LOAD_PIC(TREG)					ASM_LINE_SEP	\
+	/* pass mask as arg0 to cdisable */		ASM_LINE_SEP	\
+	ldw -24(%sp), %r26				ASM_LINE_SEP	\
+	CDISABLE					ASM_LINE_SEP	\
+	stw %ret0, -24(%sp) /* delay */			ASM_LINE_SEP	\
+	/* Restore syscall return (use arg regs) */	ASM_LINE_SEP	\
+	ldw -24(%sp), %r26				ASM_LINE_SEP	\
+	/* compare error */				ASM_LINE_SEP	\
+	ldi NO_ERROR,%r1				ASM_LINE_SEP	\
+	/* branch if no error */			ASM_LINE_SEP	\
+	cmpb,>>=,n %r1,%r26,L(pre_end)			ASM_LINE_SEP	\
+	LOAD_PIC(TREG)	/* cond. nullify */		ASM_LINE_SEP	\
+	copy %r26, TREG	/* save syscall return */	ASM_LINE_SEP	\
+	SYSCALL_ERROR_HANDLER				ASM_LINE_SEP	\
+	/* make syscall res value positive */		ASM_LINE_SEP	\
+	sub %r0, TREG, TREG	/* delay */		ASM_LINE_SEP	\
+	/* No need to LOAD_PIC */			ASM_LINE_SEP	\
+	/* store into errno location */			ASM_LINE_SEP	\
+	stw TREG, 0(%sr0,%ret0)				ASM_LINE_SEP	\
+	/* return -1 */					ASM_LINE_SEP	\
+	ldo -1(%r0), %ret0				ASM_LINE_SEP	\
+L(pre_end):						ASM_LINE_SEP	\
+	/* Restore rp before exit */			ASM_LINE_SEP	\
+	ldw -84(%sr0,%sp), %rp				ASM_LINE_SEP	\
+	/* Undo frame */				ASM_LINE_SEP	\
+	ldwm -64(%sp),TREG				ASM_LINE_SEP	\
+	/* No need to LOAD_PIC */			ASM_LINE_SEP
 
 /* Save arguments into our frame */
 # define PUSHARGS_0	/* nothing to do */
@@ -91,8 +139,8 @@
 # define PUSHARGS_2	PUSHARGS_1 stw %r25, -40(%sr0,%sp)	ASM_LINE_SEP
 # define PUSHARGS_3	PUSHARGS_2 stw %r24, -44(%sr0,%sp)	ASM_LINE_SEP
 # define PUSHARGS_4	PUSHARGS_3 stw %r23, -48(%sr0,%sp)	ASM_LINE_SEP
-# define PUSHARGS_5	PUSHARGS_4 /* Args are on the stack... */
-# define PUSHARGS_6	PUSHARGS_5
+# define PUSHARGS_5	PUSHARGS_4 stw %r22, -52(%sr0,%sp)	ASM_LINE_SEP 
+# define PUSHARGS_6	PUSHARGS_5 stw %r21, -56(%sr0,%sp)	ASM_LINE_SEP
 
 /* Bring them back from the stack */
 # define POPARGS_0	/* nothing to do */
@@ -101,7 +149,7 @@
 # define POPARGS_3	POPARGS_2 ldw -44(%sr0,%sp), %r24	ASM_LINE_SEP
 # define POPARGS_4	POPARGS_3 ldw -48(%sr0,%sp), %r23	ASM_LINE_SEP
 # define POPARGS_5	POPARGS_4 ldw -52(%sr0,%sp), %r22	ASM_LINE_SEP
-# define POPARGS_6	POPARGS_5 ldw -54(%sr0,%sp), %r21	ASM_LINE_SEP
+# define POPARGS_6	POPARGS_5 ldw -56(%sr0,%sp), %r21	ASM_LINE_SEP
 
 # ifdef IS_IN_libpthread
 #  ifdef PIC
@@ -163,10 +211,10 @@
 /* This ALT version requires newer kernel support */
 #  define SINGLE_THREAD_P_MFCTL						\
 	mfctl %cr27, %ret0					ASM_LINE_SEP	\
-	cmpib,= NO_THREAD_CR27,%ret0,Lstp			ASM_LINE_SEP	\
+	cmpib,= NO_THREAD_CR27,%ret0,L(stp)			ASM_LINE_SEP	\
 	nop							ASM_LINE_SEP	\
 	ldw MULTIPLE_THREADS_OFFSET(%sr0,%ret0),%ret0		ASM_LINE_SEP	\
- Lstp:								ASM_LINE_SEP
+L(stp):								ASM_LINE_SEP
 #  ifdef PIC
 /* Slower version uses GOT to get value of __local_multiple_threads */
 #   define SINGLE_THREAD_P							\
@@ -174,7 +222,7 @@
 	ldw RT%__local_multiple_threads(%sr0,%r1), %ret0	ASM_LINE_SEP	\
 	ldw 0(%sr0,%ret0), %ret0 				ASM_LINE_SEP
 #  else
-  /* Slow non-pic version using DP */
+/* Slow non-pic version using DP */
 #   define SINGLE_THREAD_P								\
 	addil LR%__local_multiple_threads-$global$,%r27  		ASM_LINE_SEP	\
 	ldw RR%__local_multiple_threads-$global$(%sr0,%r1),%ret0	ASM_LINE_SEP
